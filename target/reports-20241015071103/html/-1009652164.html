<html><head>
<style> table, th, td {
  border: 1px solid black;
  border-collapse: collapse;
}th {
  background: lightblue;
}</style>
</head> <body><h1>SATD</h1><table><tr><th>satd id</th> <th>satd instance id</th>  <th>project</th> <th>committer name </th> <th> Commit Hash</th> <th>old comment</th> <th>New Comment</th> <th>resolution</th> <th>Method Signature</th> <th>Method Declaration</th> <th>Method Body</th> </tr><tr><td>5562</td> <td>-1009652164</td><td>apache/hadoop</td><td>Eli Collins</td><td>a196766ea07775f18ded69bd9e8d239f8cfd3ccc</td> <td>None</td> <td>
Move the previous metafile into the current one.
</td> <td>SATD_ADDED</td> <td>thistest(Configuration, DFSTestUtil)</td> <td>private void thistest(Configuration conf, DFSTestUtil util) throws Exception</td> <td>
    MiniDFSCluster cluster = null;
    int numDataNodes = 2;
    short replFactor = 2;
    Random random = new Random();
    try {
        cluster = new MiniDFSCluster.Builder(conf).numDataNodes(numDataNodes).build();
        cluster.waitActive();
        FileSystem fs = cluster.getFileSystem();
        util.createFiles(fs, "/srcdat", replFactor);
        util.waitReplication(fs, "/srcdat", (short) 2);
        // Now deliberately remove/truncate meta blocks from the first
        // directory of the first datanode. The complete absense of a meta
        // file disallows this Datanode to send data to another datanode.
        // However, a client is alowed access to this block.
        // 
        File storageDir = MiniDFSCluster.getStorageDir(0, 1);
        String bpid = cluster.getNamesystem().getBlockPoolId();
        File data_dir = MiniDFSCluster.getFinalizedDir(storageDir, bpid);
        assertTrue("data directory does not exist", data_dir.exists());
        File[] blocks = data_dir.listFiles();
        assertTrue("Blocks do not exist in data-dir", (blocks != null) && (blocks.length > 0));
        int num = 0;
        for (int idx = 0; idx < blocks.length; idx++) {
            if (blocks[idx].getName().startsWith("blk_") && blocks[idx].getName().endsWith(".meta")) {
                num++;
                if (num % 3 == 0) {
                    // 
                    // remove .meta file
                    // 
                    System.out.println("Deliberately removing file " + blocks[idx].getName());
                    assertTrue("Cannot remove file.", blocks[idx].delete());
                } else if (num % 3 == 1) {
                    // 
                    // shorten .meta file
                    // 
                    RandomAccessFile file = new RandomAccessFile(blocks[idx], "rw");
                    FileChannel channel = file.getChannel();
                    int newsize = random.nextInt((int) channel.size() / 2);
                    System.out.println("Deliberately truncating file " + blocks[idx].getName() + " to size " + newsize + " bytes.");
                    channel.truncate(newsize);
                    file.close();
                } else {
                    // 
                    // corrupt a few bytes of the metafile
                    // 
                    RandomAccessFile file = new RandomAccessFile(blocks[idx], "rw");
                    FileChannel channel = file.getChannel();
                    long position = 0;
                    // 
                    // The very first time, corrupt the meta header at offset 0
                    // 
                    if (num != 2) {
                        position = (long) random.nextInt((int) channel.size());
                    }
                    int length = random.nextInt((int) (channel.size() - position + 1));
                    byte[] buffer = new byte[length];
                    random.nextBytes(buffer);
                    channel.write(ByteBuffer.wrap(buffer), position);
                    System.out.println("Deliberately corrupting file " + blocks[idx].getName() + " at offset " + position + " length " + length);
                    file.close();
                }
            }
        }
        // 
        // Now deliberately corrupt all meta blocks from the second
        // directory of the first datanode
        // 
        storageDir = MiniDFSCluster.getStorageDir(0, 1);
        data_dir = MiniDFSCluster.getFinalizedDir(storageDir, bpid);
        assertTrue("data directory does not exist", data_dir.exists());
        blocks = data_dir.listFiles();
        assertTrue("Blocks do not exist in data-dir", (blocks != null) && (blocks.length > 0));
        int count = 0;
        File previous = null;
        for (int idx = 0; idx < blocks.length; idx++) {
            if (blocks[idx].getName().startsWith("blk_") && blocks[idx].getName().endsWith(".meta")) {
                // 
                // Move the previous metafile into the current one.
                // 
                count++;
                if (count % 2 == 0) {
                    System.out.println("Deliberately insertimg bad crc into files " + blocks[idx].getName() + " " + previous.getName());
                    assertTrue("Cannot remove file.", blocks[idx].delete());
                    assertTrue("Cannot corrupt meta file.", previous.renameTo(blocks[idx]));
                    assertTrue("Cannot recreate empty meta file.", previous.createNewFile());
                    previous = null;
                } else {
                    previous = blocks[idx];
                }
            }
        }
        // 
        // Only one replica is possibly corrupted. The other replica should still
        // be good. Verify.
        // 
        assertTrue("Corrupted replicas not handled properly.", util.checkFiles(fs, "/srcdat"));
        System.out.println("All File still have a valid replica");
        // 
        // set replication factor back to 1. This causes only one replica of
        // of each block to remain in HDFS. The check is to make sure that
        // the corrupted replica generated above is the one that gets deleted.
        // This test is currently disabled until HADOOP-1557 is solved.
        // 
        util.setReplication(fs, "/srcdat", (short) 1);
        // util.waitReplication(fs, "/srcdat", (short)1);
        // System.out.println("All Files done with removing replicas");
        // assertTrue("Excess replicas deleted. Corrupted replicas found.",
        // util.checkFiles(fs, "/srcdat"));
        System.out.println("The excess-corrupted-replica test is disabled " + " pending HADOOP-1557");
        util.cleanup(fs, "/srcdat");
    } finally {
        if (cluster != null) {
            cluster.shutdown();
        }
    }
</td> </tr></table></body></html>