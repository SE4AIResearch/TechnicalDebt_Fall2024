<html><head>
<style> table, th, td {
  border: 1px solid black;
  border-collapse: collapse;
}th {
  background: lightblue;
}</style>
</head> <body><h1>SATD</h1><table><tr><th>satd id</th> <th>satd instance id</th>  <th>project</th> <th>committer name </th> <th> Commit Hash</th> <th>old comment</th> <th>New Comment</th> <th>resolution</th> <th>Method Signature</th> <th>Method Declaration</th> <th>Method Body</th> </tr><tr><td>5434</td> <td>-1009652292</td><td>apache/hadoop</td><td>Eli Collins</td><td>a196766ea07775f18ded69bd9e8d239f8cfd3ccc</td> <td>None</td> <td>scan all policies once every 5 second</td> <td>SATD_ADDED</td> <td>mySetup(String, int)</td> <td>private void mySetup(String erasureCode, int rsParityLength) throws Exception</td> <td>
    // Make sure data directory exists
    new File(TEST_DIR).mkdirs();
    conf = new Configuration();
    conf.set("fs.raid.recoverylogdir", LOG_DIR);
    conf.setInt(RaidNode.RS_PARITY_LENGTH_KEY, rsParityLength);
    // scan all policies once every 5 second
    conf.setLong("raid.policy.rescan.interval", 5000);
    // make all deletions not go through Trash
    conf.set("fs.shell.delete.classname", "org.apache.hadoop.hdfs.DFSClient");
    // do not use map-reduce cluster for Raiding
    conf.set("raid.classname", "org.apache.hadoop.raid.LocalRaidNode");
    conf.set("raid.server.address", "localhost:0");
    conf.setInt("hdfs.raid.stripeLength", stripeLength);
    conf.set("xor".equals(erasureCode) ? RaidNode.RAID_LOCATION_KEY : RaidNode.RAIDRS_LOCATION_KEY, "/destraid");
    dfs = new MiniDFSCluster.Builder(conf).numDataNodes(NUM_DATANODES).build();
    dfs.waitActive();
    fileSys = dfs.getFileSystem();
    namenode = fileSys.getUri().toString();
    hftp = "hftp://localhost.localdomain:" + dfs.getNameNodePort();
    FileSystem.setDefaultUri(conf, namenode);
</td> </tr></table></body></html>