<html><head>
<style> table, th, td {
  border: 1px solid black;
  border-collapse: collapse;
}th {
  background: lightblue;
}</style>
</head> <body><h1>SATD</h1><table><tr><th>satd id</th> <th>satd instance id</th>  <th>project</th> <th>committer name </th> <th> Commit Hash</th> <th>old comment</th> <th>New Comment</th> <th>resolution</th> <th>Method Signature</th> <th>Method Declaration</th> <th>Method Body</th> </tr><tr><td>5553</td> <td>-1009652173</td><td>apache/hadoop</td><td>Eli Collins</td><td>a196766ea07775f18ded69bd9e8d239f8cfd3ccc</td> <td>None</td> <td>better if we don't create the input format instance</td> <td>SATD_ADDED</td> <td>createJob(Configuration, Path[], Path, int, Shard[])</td> <td>
    // set the starting generation for each shard
    // when a reduce task fails, a new reduce task
    // has to know where to re-start
    setShardGeneration(conf, shards);
    // iconf.set sets properties in conf
    IndexUpdateConfiguration iconf = new IndexUpdateConfiguration(conf);
    Shard.setIndexShards(iconf, shards);
    // MapTask.MapOutputBuffer uses JobContext.IO_SORT_MB to decide its max buffer size
    // (max buffer size = 1/2 * JobContext.IO_SORT_MB).
    // Here we half-en JobContext.IO_SORT_MB because we use the other half memory to
    // build an intermediate form/index in Combiner.
    iconf.setIOSortMB(iconf.getIOSortMB() / 2);
    // create the job configuration
    JobConf jobConf = new JobConf(conf, IndexUpdater.class);
    jobConf.setJobName(this.getClass().getName() + "_" + System.currentTimeMillis());
    // provided by application
    FileInputFormat.setInputPaths(jobConf, inputPaths);
    FileOutputFormat.setOutputPath(jobConf, outputPath);
    jobConf.setNumMapTasks(numMapTasks);
    // already set shards
    jobConf.setNumReduceTasks(shards.length);
    jobConf.setInputFormat(iconf.getIndexInputFormatClass());
    Path[] inputs = FileInputFormat.getInputPaths(jobConf);
    StringBuilder buffer = new StringBuilder(inputs[0].toString());
    for (int i = 1; i < inputs.length; i++) {
        buffer.append(",");
        buffer.append(inputs[i].toString());
    }
    LOG.info("mapred.input.dir = " + buffer.toString());
    LOG.info("mapreduce.output.fileoutputformat.outputdir = " + FileOutputFormat.getOutputPath(jobConf).toString());
    LOG.info("mapreduce.job.maps = " + jobConf.getNumMapTasks());
    LOG.info("mapreduce.job.reduces = " + jobConf.getNumReduceTasks());
    LOG.info(shards.length + " shards = " + iconf.getIndexShards());
    // better if we don't create the input format instance
    LOG.info("mapred.input.format.class = " + jobConf.getInputFormat().getClass().getName());
    // set by the system
    jobConf.setMapOutputKeyClass(IndexUpdateMapper.getMapOutputKeyClass());
    jobConf.setMapOutputValueClass(IndexUpdateMapper.getMapOutputValueClass());
    jobConf.setOutputKeyClass(IndexUpdateReducer.getOutputKeyClass());
    jobConf.setOutputValueClass(IndexUpdateReducer.getOutputValueClass());
    jobConf.setMapperClass(IndexUpdateMapper.class);
    jobConf.setPartitionerClass(IndexUpdatePartitioner.class);
    jobConf.setCombinerClass(IndexUpdateCombiner.class);
    jobConf.setReducerClass(IndexUpdateReducer.class);
    jobConf.setOutputFormat(IndexUpdateOutputFormat.class);
    return jobConf;
</td> <td> JobConf createJob(Configuration conf, Path[] inputPaths, Path outputPath, int numMapTasks, Shard[] shards) throws IOException</td> </tr></table></body></html>