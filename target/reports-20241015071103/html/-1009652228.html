<html><head>
<style> table, th, td {
  border: 1px solid black;
  border-collapse: collapse;
}th {
  background: lightblue;
}</style>
</head> <body><h1>SATD</h1><table><tr><th>satd id</th> <th>satd instance id</th>  <th>project</th> <th>committer name </th> <th> Commit Hash</th> <th>old comment</th> <th>New Comment</th> <th>resolution</th> <th>Method Signature</th> <th>Method Declaration</th> <th>Method Body</th> </tr><tr><td>5498</td> <td>-1009652228</td><td>apache/hadoop</td><td>Eli Collins</td><td>a196766ea07775f18ded69bd9e8d239f8cfd3ccc</td> <td>None</td> <td>move the blocks</td> <td>SATD_ADDED</td> <td>testConcatNotCompleteBlock()</td> <td>public void testConcatNotCompleteBlock() throws IOException</td> <td>
    long trgFileLen = blockSize * 3;
    // block at the end - not full
    long srcFileLen = blockSize * 3 + 20;
    // create first file
    String name1 = "/trg", name2 = "/src";
    Path filePath1 = new Path(name1);
    DFSTestUtil.createFile(dfs, filePath1, trgFileLen, REPL_FACTOR, 1);
    HdfsFileStatus fStatus = cluster.getNameNode().getFileInfo(name1);
    long fileLen = fStatus.getLen();
    assertEquals(fileLen, trgFileLen);
    // read the file
    FSDataInputStream stm = dfs.open(filePath1);
    byte[] byteFile1 = new byte[(int) trgFileLen];
    stm.readFully(0, byteFile1);
    stm.close();
    LocatedBlocks lb1 = cluster.getNameNode().getBlockLocations(name1, 0, trgFileLen);
    Path filePath2 = new Path(name2);
    DFSTestUtil.createFile(dfs, filePath2, srcFileLen, REPL_FACTOR, 1);
    fStatus = cluster.getNameNode().getFileInfo(name2);
    fileLen = fStatus.getLen();
    assertEquals(srcFileLen, fileLen);
    // read the file
    stm = dfs.open(filePath2);
    byte[] byteFile2 = new byte[(int) srcFileLen];
    stm.readFully(0, byteFile2);
    stm.close();
    LocatedBlocks lb2 = cluster.getNameNode().getBlockLocations(name2, 0, srcFileLen);
    System.out.println("trg len=" + trgFileLen + "; src len=" + srcFileLen);
    // move the blocks
    dfs.concat(filePath1, new Path[] { filePath2 });
    long totalLen = trgFileLen + srcFileLen;
    fStatus = cluster.getNameNode().getFileInfo(name1);
    fileLen = fStatus.getLen();
    // read the resulting file
    stm = dfs.open(filePath1);
    byte[] byteFileConcat = new byte[(int) fileLen];
    stm.readFully(0, byteFileConcat);
    stm.close();
    LocatedBlocks lbConcat = cluster.getNameNode().getBlockLocations(name1, 0, fileLen);
    // verifications
    // 1. number of blocks
    assertEquals(lbConcat.locatedBlockCount(), lb1.locatedBlockCount() + lb2.locatedBlockCount());
    // 2. file lengths
    System.out.println("file1 len=" + fileLen + "; total len=" + totalLen);
    assertEquals(fileLen, totalLen);
    // 3. removal of the src file
    fStatus = cluster.getNameNode().getFileInfo(name2);
    // file shouldn't exist
    assertNull("File " + name2 + "still exists", fStatus);
    // 4. content
    checkFileContent(byteFileConcat, new byte[][] { byteFile1, byteFile2 });
</td> </tr></table></body></html>