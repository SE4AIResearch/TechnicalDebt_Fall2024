<html><head>
<style> table, th, td {
  border: 1px solid black;
  border-collapse: collapse;
}th {
  background: lightblue;
}</style>
</head> <body><h1>SATD</h1><table><tr><th>satd id</th> <th>satd instance id</th>  <th>project</th> <th>committer name </th> <th> Commit Hash</th> <th>old comment</th> <th>New Comment</th> <th>resolution</th> <th>Method Signature</th> <th>Method Declaration</th> <th>Method Body</th> </tr><tr><td>5574</td> <td>-1009652152</td><td>apache/hadoop</td><td>Eli Collins</td><td>a196766ea07775f18ded69bd9e8d239f8cfd3ccc</td> <td>None</td> <td>Don't assign map tasks to the hilt!
Leave some free slots in the cluster for future task-failures,
speculative tasks etc. beyond the highest priority job</td> <td>SATD_ADDED</td> <td>assignTasks(TaskTracker)</td> <td>public synchronized List<Task> assignTasks(TaskTracker taskTracker) throws IOException</td> <td>
    TaskTrackerStatus taskTrackerStatus = taskTracker.getStatus();
    ClusterStatus clusterStatus = taskTrackerManager.getClusterStatus();
    final int numTaskTrackers = clusterStatus.getTaskTrackers();
    final int clusterMapCapacity = clusterStatus.getMaxMapTasks();
    final int clusterReduceCapacity = clusterStatus.getMaxReduceTasks();
    Collection<JobInProgress> jobQueue = jobQueueJobInProgressListener.getJobQueue();
    // 
    // Get map + reduce counts for the current tracker.
    // 
    final int trackerMapCapacity = taskTrackerStatus.getMaxMapSlots();
    final int trackerReduceCapacity = taskTrackerStatus.getMaxReduceSlots();
    final int trackerRunningMaps = taskTrackerStatus.countMapTasks();
    final int trackerRunningReduces = taskTrackerStatus.countReduceTasks();
    // Assigned tasks
    List<Task> assignedTasks = new ArrayList<Task>();
    // 
    // Compute (running + pending) map and reduce task numbers across pool
    // 
    int remainingReduceLoad = 0;
    int remainingMapLoad = 0;
    synchronized (jobQueue) {
        for (JobInProgress job : jobQueue) {
            if (job.getStatus().getRunState() == JobStatus.RUNNING) {
                remainingMapLoad += (job.desiredMaps() - job.finishedMaps());
                if (job.scheduleReduces()) {
                    remainingReduceLoad += (job.desiredReduces() - job.finishedReduces());
                }
            }
        }
    }
    // Compute the 'load factor' for maps and reduces
    double mapLoadFactor = 0.0;
    if (clusterMapCapacity > 0) {
        mapLoadFactor = (double) remainingMapLoad / clusterMapCapacity;
    }
    double reduceLoadFactor = 0.0;
    if (clusterReduceCapacity > 0) {
        reduceLoadFactor = (double) remainingReduceLoad / clusterReduceCapacity;
    }
    // 
    // In the below steps, we allocate first map tasks (if appropriate),
    // and then reduce tasks if appropriate.  We go through all jobs
    // in order of job arrival; jobs only get serviced if their
    // predecessors are serviced, too.
    // 
    // 
    // We assign tasks to the current taskTracker if the given machine
    // has a workload that's less than the maximum load of that kind of
    // task.
    // However, if the cluster is close to getting loaded i.e. we don't
    // have enough _padding_ for speculative executions etc., we only
    // schedule the "highest priority" task i.e. the task from the job
    // with the highest priority.
    // 
    final int trackerCurrentMapCapacity = Math.min((int) Math.ceil(mapLoadFactor * trackerMapCapacity), trackerMapCapacity);
    int availableMapSlots = trackerCurrentMapCapacity - trackerRunningMaps;
    boolean exceededMapPadding = false;
    if (availableMapSlots > 0) {
        exceededMapPadding = exceededPadding(true, clusterStatus, trackerMapCapacity);
    }
    int numLocalMaps = 0;
    int numNonLocalMaps = 0;
    scheduleMaps: for (int i = 0; i < availableMapSlots; ++i) {
        synchronized (jobQueue) {
            for (JobInProgress job : jobQueue) {
                if (job.getStatus().getRunState() != JobStatus.RUNNING) {
                    continue;
                }
                Task t = null;
                // Try to schedule a node-local or rack-local Map task
                t = job.obtainNewLocalMapTask(taskTrackerStatus, numTaskTrackers, taskTrackerManager.getNumberOfUniqueHosts());
                if (t != null) {
                    assignedTasks.add(t);
                    ++numLocalMaps;
                    // Don't assign map tasks to the hilt!
                    // Leave some free slots in the cluster for future task-failures,
                    // speculative tasks etc. beyond the highest priority job
                    if (exceededMapPadding) {
                        break scheduleMaps;
                    }
                    // Try all jobs again for the next Map task
                    break;
                }
                // Try to schedule a node-local or rack-local Map task
                t = job.obtainNewNonLocalMapTask(taskTrackerStatus, numTaskTrackers, taskTrackerManager.getNumberOfUniqueHosts());
                if (t != null) {
                    assignedTasks.add(t);
                    ++numNonLocalMaps;
                    // We assign at most 1 off-switch or speculative task
                    // This is to prevent TaskTrackers from stealing local-tasks
                    // from other TaskTrackers.
                    break scheduleMaps;
                }
            }
        }
    }
    int assignedMaps = assignedTasks.size();
    // 
    // Same thing, but for reduce tasks
    // However we _never_ assign more than 1 reduce task per heartbeat
    // 
    final int trackerCurrentReduceCapacity = Math.min((int) Math.ceil(reduceLoadFactor * trackerReduceCapacity), trackerReduceCapacity);
    final int availableReduceSlots = Math.min((trackerCurrentReduceCapacity - trackerRunningReduces), 1);
    boolean exceededReducePadding = false;
    if (availableReduceSlots > 0) {
        exceededReducePadding = exceededPadding(false, clusterStatus, trackerReduceCapacity);
        synchronized (jobQueue) {
            for (JobInProgress job : jobQueue) {
                if (job.getStatus().getRunState() != JobStatus.RUNNING || job.numReduceTasks == 0) {
                    continue;
                }
                Task t = job.obtainNewReduceTask(taskTrackerStatus, numTaskTrackers, taskTrackerManager.getNumberOfUniqueHosts());
                if (t != null) {
                    assignedTasks.add(t);
                    break;
                }
                // Don't assign reduce tasks to the hilt!
                // Leave some free slots in the cluster for future task-failures,
                // speculative tasks etc. beyond the highest priority job
                if (exceededReducePadding) {
                    break;
                }
            }
        }
    }
    if (LOG.isDebugEnabled()) {
        LOG.debug("Task assignments for " + taskTrackerStatus.getTrackerName() + " --> " + "[" + mapLoadFactor + ", " + trackerMapCapacity + ", " + trackerCurrentMapCapacity + ", " + trackerRunningMaps + "] -> [" + (trackerCurrentMapCapacity - trackerRunningMaps) + ", " + assignedMaps + " (" + numLocalMaps + ", " + numNonLocalMaps + ")] [" + reduceLoadFactor + ", " + trackerReduceCapacity + ", " + trackerCurrentReduceCapacity + "," + trackerRunningReduces + "] -> [" + (trackerCurrentReduceCapacity - trackerRunningReduces) + ", " + (assignedTasks.size() - assignedMaps) + "]");
    }
    return assignedTasks;
</td> </tr></table></body></html>