<html><head>
<style> table, th, td {
  border: 1px solid black;
  border-collapse: collapse;
}th {
  background: lightblue;
}</style>
</head> <body><h1>SATD</h1><table><tr><th>satd id</th> <th>satd instance id</th>  <th>project</th> <th>committer name </th> <th> Commit Hash</th> <th>old comment</th> <th>New Comment</th> <th>resolution</th> <th>Method Signature</th> <th>Method Declaration</th> <th>Method Body</th> </tr><tr><td>5411</td> <td>-1009652315</td><td>apache/hadoop</td><td>Eli Collins</td><td>a196766ea07775f18ded69bd9e8d239f8cfd3ccc</td> <td>None</td> <td>This kind of error doesn't mean that the stream itself is broken - just the
flushing thread got interrupted. So, we shouldn't close down the writer,
but instead just propagate the error</td> <td>SATD_ADDED</td> <td>hflush()</td> <td>public void hflush() throws IOException</td> <td>
    dfsClient.checkOpen();
    isClosed();
    try {
        long toWaitFor;
        synchronized (this) {
            /* Record current blockOffset. This might be changed inside
         * flushBuffer() where a partial checksum chunk might be flushed.
         * After the flush, reset the bytesCurBlock back to its previous value,
         * any partial checksum chunk will be sent now and in next packet.
         */
            long saveOffset = bytesCurBlock;
            Packet oldCurrentPacket = currentPacket;
            // flush checksum buffer, but keep checksum buffer intact
            flushBuffer(true);
            // bytesCurBlock potentially incremented if there was buffered data
            if (DFSClient.LOG.isDebugEnabled()) {
                DFSClient.LOG.debug("DFSClient flush() : saveOffset " + saveOffset + " bytesCurBlock " + bytesCurBlock + " lastFlushOffset " + lastFlushOffset);
            }
            // Flush only if we haven't already flushed till this offset.
            if (lastFlushOffset != bytesCurBlock) {
                assert bytesCurBlock > lastFlushOffset;
                // record the valid offset of this flush
                lastFlushOffset = bytesCurBlock;
                waitAndQueueCurrentPacket();
            } else {
                // We already flushed up to this offset.
                // This means that we haven't written anything since the last flush
                // (or the beginning of the file). Hence, we should not have any
                // packet queued prior to this call, since the last flush set
                // currentPacket = null.
                assert oldCurrentPacket == null : "Empty flush should not occur with a currentPacket";
                // just discard the current packet since it is already been sent.
                currentPacket = null;
            }
            // Restore state of stream. Record the last flush offset
            // of the last full chunk that was flushed.
            // 
            bytesCurBlock = saveOffset;
            toWaitFor = lastQueuedSeqno;
        }
        // end synchronized
        waitForAckedSeqno(toWaitFor);
        // If any new blocks were allocated since the last flush,
        // then persist block locations on namenode.
        // 
        if (persistBlocks.getAndSet(false)) {
            try {
                dfsClient.namenode.fsync(src, dfsClient.clientName);
            } catch (IOException ioe) {
                DFSClient.LOG.warn("Unable to persist blocks in hflush for " + src, ioe);
                // If we got an error here, it might be because some other thread called
                // close before our hflush completed. In that case, we should throw an
                // exception that the stream is closed.
                isClosed();
                // If we aren't closed but failed to sync, we should expose that to the
                // caller.
                throw ioe;
            }
        }
        synchronized (this) {
            if (streamer != null) {
                streamer.setHflush();
            }
        }
    } catch (InterruptedIOException interrupt) {
        // This kind of error doesn't mean that the stream itself is broken - just the
        // flushing thread got interrupted. So, we shouldn't close down the writer,
        // but instead just propagate the error
        throw interrupt;
    } catch (IOException e) {
        DFSClient.LOG.warn("Error while syncing", e);
        synchronized (this) {
            if (!closed) {
                lastException = new IOException("IOException flush:" + e);
                closeThreads(true);
            }
        }
        throw e;
    }
</td> </tr></table></body></html>