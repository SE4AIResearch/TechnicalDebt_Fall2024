<html><head>
<style> table, th, td {
  border: 1px solid black;
  border-collapse: collapse;
}th {
  background: lightblue;
}</style>
</head> <body><h1>SATD</h1><table><tr><th>satd id</th> <th>satd instance id</th>  <th>project</th> <th>committer name </th> <th> Commit Hash</th> <th>old comment</th> <th>New Comment</th> <th>resolution</th> <th>Method Signature</th> <th>Method Declaration</th> <th>Method Body</th> </tr><tr><td>5447</td> <td>-1009652279</td><td>apache/hadoop</td><td>Eli Collins</td><td>a196766ea07775f18ded69bd9e8d239f8cfd3ccc</td> <td>None</td> <td>u1 has 5 map slots and 5 reduce slots. u2 has 4 map slots and 4 reduce
slots. Because of high memory tasks, giving u2 another task would
overflow limits. So, no more tasks should be given to anyone.</td> <td>SATD_ADDED</td> <td>testUserLimitsForHighMemoryJobs()</td> <td>public void testUserLimitsForHighMemoryJobs() throws IOException</td> <td>
    taskTrackerManager = new FakeTaskTrackerManager(1, 10, 10);
    scheduler.setTaskTrackerManager(taskTrackerManager);
    String[] qs = { "default" };
    taskTrackerManager.addQueues(qs);
    ArrayList<FakeQueueInfo> queues = new ArrayList<FakeQueueInfo>();
    queues.add(new FakeQueueInfo("default", 100.0f, true, 50));
    // enabled memory-based scheduling
    // Normal job in the cluster would be 1GB maps/reduces
    scheduler.getConf().setLong(JTConfig.JT_MAX_MAPMEMORY_MB, 2 * 1024);
    scheduler.getConf().setLong(MRConfig.MAPMEMORY_MB, 1 * 1024);
    scheduler.getConf().setLong(JTConfig.JT_MAX_REDUCEMEMORY_MB, 2 * 1024);
    scheduler.getConf().setLong(MRConfig.REDUCEMEMORY_MB, 1 * 1024);
    taskTrackerManager.setFakeQueues(queues);
    scheduler.start();
    // Submit one normal job to the other queue.
    JobConf jConf = new JobConf(conf);
    jConf.setMemoryForMapTask(1 * 1024);
    jConf.setMemoryForReduceTask(1 * 1024);
    jConf.setNumMapTasks(6);
    jConf.setNumReduceTasks(6);
    jConf.setUser("u1");
    jConf.setQueueName("default");
    FakeJobInProgress job1 = taskTrackerManager.submitJobAndInit(JobStatus.PREP, jConf);
    LOG.debug("Submit one high memory(2GB maps, 2GB reduces) job of " + "6 map and 6 reduce tasks");
    jConf = new JobConf(conf);
    jConf.setMemoryForMapTask(2 * 1024);
    jConf.setMemoryForReduceTask(2 * 1024);
    jConf.setNumMapTasks(6);
    jConf.setNumReduceTasks(6);
    jConf.setQueueName("default");
    jConf.setUser("u2");
    FakeJobInProgress job2 = taskTrackerManager.submitJobAndInit(JobStatus.PREP, jConf);
    // Verify that normal job takes 5 task assignments to hit user limits
    Map<String, String> expectedStrings = new HashMap<String, String>();
    for (int i = 0; i < 5; i++) {
        expectedStrings.clear();
        expectedStrings.put(CapacityTestUtils.MAP, "attempt_test_0001_m_00000" + (i + 1) + "_0 on tt1");
        expectedStrings.put(CapacityTestUtils.REDUCE, "attempt_test_0001_r_00000" + (i + 1) + "_0 on tt1");
        checkMultipleTaskAssignment(taskTrackerManager, scheduler, "tt1", expectedStrings);
    }
    // u1 has 5 map slots and 5 reduce slots. u2 has none. So u1's user limits
    // are hit. So u2 should get slots
    for (int i = 0; i < 2; i++) {
        expectedStrings.clear();
        expectedStrings.put(CapacityTestUtils.MAP, "attempt_test_0002_m_00000" + (i + 1) + "_0 on tt1");
        expectedStrings.put(CapacityTestUtils.REDUCE, "attempt_test_0002_r_00000" + (i + 1) + "_0 on tt1");
        checkMultipleTaskAssignment(taskTrackerManager, scheduler, "tt1", expectedStrings);
    }
    // u1 has 5 map slots and 5 reduce slots. u2 has 4 map slots and 4 reduce
    // slots. Because of high memory tasks, giving u2 another task would
    // overflow limits. So, no more tasks should be given to anyone.
    assertNull(scheduler.assignTasks(tracker("tt1")));
</td> </tr></table></body></html>